---
title: "Machine Learning Course Project"
author: "Sergio Garcia Busto"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, echo=FALSE, message=FALSE, results='hide'}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(DataExplorer)
library(caret)
set.seed(2525)
library(parallel)
library(doParallel)
```

# Introduction

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. However, the performance of such activities is rarely quantified. The data used in this project was recorded via accelerometers on the belt, forearm, arm, and dumbell of 6 participants that were asked to perform barbell lifts correctly and incorrectly in 5 different ways (A-E, from best to worst). The goal, therefore, is to use the collected data to predict how well the movement is performed using machine learning (ML). More information on the data is available from the website here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). It should be noted that although data was measured overtime, making this dataset a time series, the ML models built will not account for this and only the continuous variables will be used.

The training data for this project are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
The test data are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv


```{r}
#load data
training.raw<-read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"))
testing.raw<-read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"))
```

# Exploratory Data Analysis (EDA) and data wrangling

An initial overview of the data can be conducted via the `View`, `head` or `glimpse` functions. The dataset contains 19,622 data entries or rows (20 for the test dataset) and 160 variables or columns. However, it is clear from the data overview shown below that some variables are not needed for model building. 

```{r}
glimpse(training.raw)
```

Particularly, variables that describe the distribution of other variables, namely skewness, kurtosis, average, max/min, standard deviation and variance are not needed and contain a large proportion of missing values. Moreover, as explained in the introduction, variables that relate to data collection (such as timepoints and subject names) will be removed. In order to assess model via the out of sample accuracy, the training dataset will be split into a training and test dataset, making the preliminary test set the validation set. Prediction accuracy will be additionally measured using the validation set in the course quiz.

```{r}
sep<-createDataPartition(y=training.raw$classe, p=0.75, list=FALSE)
training<-training.raw[sep,]
testing<-training.raw[-sep,]
```

```{r}
#exclude parameters that summarise info and have missing values #omit time and index
training.wrangle<-data.frame(training[,-grep("^max|^min|^kurtosis|^skewness|^amplitude|^var|^stddev|^avg|^new|^num|^cvtd|^raw_timestamp|^X|^user_name", colnames(training))])
#the same is done in the test dataset
testing.wrangle<-data.frame(testing[,-grep("^max|^min|^kurtosis|^skewness|^amplitude|^var|^stddev|^avg|^new|^num|^cvtd|^raw_timestamp|^X|^user_name", colnames(testing))])

validation<-data.frame(testing.raw[,-grep("^max|^min|^kurtosis|^skewness|^amplitude|^var|^stddev|^avg|^new|^num|^cvtd|^raw_timestamp|^X|^user_name|^problem_id", colnames(testing.raw))])

#check there are no missing values
sum(colSums(is.na(training.wrangle)) >=1)
```

None of the remaining variables have near-zero-variance, so ideally, all of them can potentially be accounted for in the model. Histograms of the relevant variables show that some distributions are far from normal; in fact, some are bimodal and others are highly skewed. Although preprocessing can be done to tackle these issues, given that the outcome predictor is a class (how well each movement is performed), methods that do not require normality of predictors should be used, such as decision trees and random forests. Importantly, hey have proven to be highly accurate for predicting categorical variables using high-dimensional data. Random forests were selected for the purpose of this project (see next section).

```{r}
plot_histogram(training.wrangle)
var<-nearZeroVar(training.wrangle, saveMetrics = TRUE)
sum(var$nzv)
```

# Principal Component Analysis (PCA)

Dimensionality reduction of datasets with a high number of predictors can be done via PCA. Although it can be included as part of the preprocessing when fitting a ML model, it will be performed separately below for data exploration purposes. Given the fact that random forests readily make accurate predictions when using data with these characteristics, 95% of the variance will be captured in PCA, which was accomplished using 25 principal components.

```{r}
#preprocessing with pca, create a new dataset with the names for subsequent exploration

training.wrangle.name<-data.frame(training[,-grep("^max|^min|^kurtosis|^skewness|^amplitude|^var|^stddev|^avg|^new|^num|^cvtd|^raw_timestamp|^X", colnames(training))])

preProc<-preProcess(training.wrangle.name[,2:53], method="pca", thresh = 0.95)
predpca<-predict(preProc, training.wrangle.name[,2:53])
preProc
```

When plotting the first and second principal components one can clearly see that most of the variation can be attributed to five distinct groups or clusters. Interestingly, when the data points are coloured by movement performance or individual, it seems like most variation in the data corresponds to different individuals/subjects (as expected), and that differences in movement performance are much slighter. This does not give an indication of whether fitting the model with PCA as part of the preprocessing will benefit accuracy; therefore, models with and without PCA will be fitted.

```{r}
typeColor<-((training.wrangle.name$classe=="A")*7+(training.wrangle.name$classe=="E")*11+(training.wrangle.name$classe=="B")*8+(training.wrangle.name$classe=="C")*9+(training.wrangle.name$classe=="D")*10)
typeColorfac<-as.factor(typeColor)

training.wrangle.name$user_name<-as.factor(training.wrangle.name$user_name)
typeColor2<-((training.wrangle.name$user_name=="adelmo")*1+(training.wrangle.name$user_name=="carlitos")*5+(training.wrangle.name$user_name=="charles")*2+(training.wrangle.name$user_name=="eurico")*3+(training.wrangle.name$user_name=="jeremy")*4+(training.wrangle.name$user_name=="pedro")*6)
typeColor2fac<-as.factor(typeColor2)

gppl<-ggplot(data=predpca, aes(x=predpca[,1], y=predpca[,2], color=typeColor2fac))+geom_point(size=3, alpha=0.2)+labs(title="PCA by subjects", x="PC1", y="PC2", color="Individual")+scale_color_manual(labels=c("Adelmo",  "Charles", "Eurico", "Jeremy","Carlitos", "Pedro"), values = c(1, 2, 3,4,5,6))
gppl

gqual<-ggplot(data=predpca, aes(x=predpca[,1], y=predpca[,2], color=typeColorfac))+geom_point(size=3, alpha=0.2)+labs(title="PCA by movement performance", x="PC1", y="PC2", color="Grade")+scale_color_manual(labels=c("A",  "B", "C", "D","E"), values = c(11,12,9,10,7))
gqual
```

# Model building using random forests

Using the `caret` package, two models (one with PCA as part of the preprocessing and one without) were built. Several adjustments needed to be made in order to increase computing efficiency. These include using 5-fold cross-validation as a resampling method, which is significantly less computationally intensive than the standard bootstraping (though there is a slight accuracy tradeoff). In short, a fifth of the training data is sequentially used for the internal averaging of model inaccuracies, whereas bootstraping resamples the entire training data with replacement for averaging such differences. Plots of both models show that the model with no preprocessing achieves a higher accuracy than the model with preprocessing.

```{r}
#rf with pca or without pca
  #make predictions

#use https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-randomForestPerformance.md to improve processing time, if not it's impossible

cluster<-makeCluster(detectCores() - 1)
registerDoParallel(cluster)
fitControl <- trainControl(method = "cv",number = 5,allowParallel = TRUE)

tr.y<-training.wrangle[,53]
tr.x<-training.wrangle[,-53]
modrf<-train(tr.x,tr.y, method="rf", data = training.wrangle, trControl = fitControl)
modrfpca<-train(tr.x,tr.y, method="rf", data = training.wrangle, preProcess = "pca", trControl = fitControl)

plot(modrf)
plot(modrfpca)
```


## Out of sample accuracy and predictions

Using the test data stemming from the `training.raw` dataset, the out of sample accuracy or accuracy of either model can be estimated. This also serves to objectively measure the accuracy of the models using new data, as the redistribution error is optimistic due to overfitting. Again, the model trained using the raw data shows the highest accuracy with a value of 99.6% (95% CI 99.4-99.8) compared to the model trained using pre-processed data (97.8%, CI 97.4-98.2). 

```{r}
predrf<-predict(modrf, testing.wrangle)
predrfpca<-predict(modrfpca, testing.wrangle)
confusionMatrix(predrf, as.factor(testing.wrangle$classe))
confusionMatrix(predrfpca, as.factor(testing.wrangle$classe))
```

Using the validation set, 20 predictions were done with both models. Despite the higher accuracy of the model with no preprocessing when using the test set, both were accurate enough to predict the 20 samples correctly, therefore acheiving 100% accuracy in the validation set.

```{r}
valrf<-predict(modrf, validation)
valrfpca<-predict(modrfpca, validation)
answers.test<-as.factor(c("B", "A", "B", "A", "A", "E", "D", "B", "A", "A","B", "C", "B", "A", "E", "E", "A", "B", "B", "B"))
#predictions from both models are correct
table(valrf, answers.test)
table(valrfpca, answers.test)
```

## Conclusion

In this project, two successful ML models were built that predict movement performance from data measured using accelerometers. The models were built using random forests and 5-fold cross-validation was chosen for resampling. A model was trained using preprocessed data via PCA, while the other was trained using raw data. The former achieved a 99.6% accuracy, while the latter had 97.8%. However, both were accurate enough to correctly predict the 20 samples in the validation set.